{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import random\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "train = sp500.iloc[:-100]\n",
    "test = sp500.iloc[-100:]\n",
    "\n",
    "# Define the hyperparameters to optimize\n",
    "etas = [0.01,0.05,0.10,0.20,0.30,0.40]\n",
    "\n",
    "params = {\n",
    "    'max_depth': [5, 10, 15, 20,25],\n",
    "    'subsample': [0.5,0.6,0.7,0.8,0.9,1.0],\n",
    "    'colsample_bytree': [0.5,0.6,0.7,0.8,0.9,1.0],\n",
    "    'gamma' : [0.00,0.05,0.10,0.15,0.20],\n",
    "    'scale_pos_weight' : [30,40,50,300,400,500,600,700], \n",
    "}\n",
    "\n",
    "iSol = {}\n",
    "for key,values in params.items():\n",
    "    iSol[key] = random.choice(values)\n",
    "\n",
    "cSol = iSol\n",
    "tStart = 100\n",
    "tEnd = 1e-3\n",
    "nLoops = 30 \n",
    "\n",
    "\n",
    "frac = (tEnd/tStart)**(1.0/(nLoops-1.0))\n",
    "\n",
    "# Define the objective function to minimize\n",
    "\n",
    "\n",
    "def predict(train, test, predictors, model):\n",
    "    model.fit(train[predictors], train[\"Target\"])\n",
    "    preds = model.predict(test[predictors])\n",
    "    preds = pd.Series(preds, index=test.index, name=\"Predictions\")\n",
    "    combined = pd.concat([test[\"Target\"], preds], axis=1)\n",
    "    return combined\n",
    "\n",
    "\n",
    "def backtest(data, model, predictors, start=2500, step=250):\n",
    "    all_predictions = []\n",
    "\n",
    "    for i in range(start, data.shape[0], step):\n",
    "        train = data.iloc[0:i].copy()\n",
    "        test = data.iloc[i:(i+step)].copy()\n",
    "        predictions = predict(train, test, predictors, model)\n",
    "        all_predictions.append(predictions)\n",
    "    \n",
    "    return pd.concat(all_predictions)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def objective_function(cSol):\n",
    "    # Create a new XGBoost model with the given hyperparameters\n",
    "    model = XGBClassifier(**cSol)\n",
    "    \n",
    "    # Train the model on the training data\n",
    "    predictors = [\"Close\", \"Volume\", \"Open\", \"High\", \"Low\"]\n",
    "    model.fit(train[predictors], train[\"Target\"])\n",
    "    \n",
    "    # Calculate the mean squared error\n",
    "    predictions = backtest(sp500, model, predictors)\n",
    "    \n",
    "    return precision_score(predictions[\"Target\"], predictions[\"Predictions\"])\n",
    "\n",
    "# Define the Simulated Annealing algorithm\n",
    "def simulated_annealing(T, frac, n):\n",
    "    # Initialize the current solution\n",
    "    current_solution = {}\n",
    "\n",
    "    \n",
    "\n",
    "    for key,values in params.items():\n",
    "        current_solution[key] = random.choice(values)\n",
    "        current_solution.update({'eta':0.001})\n",
    "\n",
    "    # Calculate the initial objective function value\n",
    "    current_obj_val = objective_function(current_solution)\n",
    "    \n",
    "    # Iterate for n iterations\n",
    "    for i in range(n):\n",
    "        # Generate a new solution\n",
    "        new_solution = {}\n",
    "        for  eta in etas:\n",
    "            for key,values in params.items():\n",
    "                new_solution[key] = random.choice(values)\n",
    "                new_solution.update({'eta':eta})\n",
    "            \n",
    "            # Calculate the new objective function value\n",
    "            new_obj_val = objective_function(new_solution)\n",
    "\n",
    "            if new_obj_val>current_obj_val:\n",
    "                current_obj_val = new_obj_val\n",
    "                current_solution = new_solution\n",
    "            elif new_obj_val <= current_obj_val:\n",
    "                # Calculate the probability of accepting the new solution\n",
    "                p = np.exp(-(current_obj_val - new_obj_val) / T)\n",
    "                # Accept the new solution with probability p\n",
    "                if np.random.uniform(0, 1) < p:\n",
    "                    current_solution = new_solution\n",
    "                    current_obj_val = new_obj_val\n",
    "            \n",
    "            # Decrease the temperature\n",
    "            T *= frac\n",
    "            \n",
    "            # Check if the temperature has reached the minimum value\n",
    "            if T < tEnd:\n",
    "                break\n",
    "            print(\"Current Temp\", T)\n",
    "            print(\"Current Hyperparameters: \",current_solution)\n",
    "            print(\"Current Objective Value: \",current_obj_val)\n",
    "            \n",
    "    \n",
    "    # Return the best solution and its objective function value\n",
    "    return current_solution, current_obj_val,T\n",
    "\n",
    "# Run the Simulated Annealing algorithm\n",
    "best_solution, best_obj_val = simulated_annealing(tStart, frac, nLoops)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "#print('Best Hyperparameters:', {k: params[k][np.argmax(best_solution[i])] for i, k in enumerate(params)})\n",
    "#print('Best Objective Function Value:', best_obj_val)\n",
    "print(\"Best Hyperparameters : \", best_solution)\n",
    "print(\"Best Objective Function Value : \",best_obj_val)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
